{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Scraping \n",
    "\n",
    "cellEnabled = 1\n",
    "\n",
    "if cellEnabled:\n",
    "    url = (\"https://finviz.com/quote.ashx?t=META\")\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    with open(os.path.join(\"data\", 'meta.html'), \"w\", encoding = 'utf-8') as file:\n",
    "        # prettify the soup object and convert it into a string\n",
    "        #file.write(str(soup.prettify()))\n",
    "        file.write(str(webpage))\n",
    "\n",
    "    url = (\"https://finviz.com/quote.ashx?t=AMZN\")\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    with open(os.path.join(\"data\", 'amazon.html'), \"w\", encoding = 'utf-8') as file:\n",
    "        # prettify the soup object and convert it into a string\n",
    "        #file.write(str(soup.prettify()))\n",
    "        file.write(str(webpage))\n",
    "\n",
    "    url = (\"https://finviz.com/quote.ashx?t=AAPL\")\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    with open(os.path.join(\"data\", 'apple.html'), \"w\", encoding = 'utf-8') as file:\n",
    "        # prettify the soup object and convert it into a string\n",
    "        #file.write(str(soup.prettify()))\n",
    "        file.write(str(webpage))\n",
    "\n",
    "    url = (\"https://finviz.com/quote.ashx?t=NFLX\")\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    with open(os.path.join(\"data\", 'netflix.html'), \"w\", encoding = 'utf-8') as file:\n",
    "        # prettify the soup object and convert it into a string\n",
    "        #file.write(str(soup.prettify()))\n",
    "        file.write(str(webpage))\n",
    "\n",
    "    url = (\"https://finviz.com/quote.ashx?t=GOOGL\")\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    with open(os.path.join(\"data\", 'google.html'), \"w\", encoding = 'utf-8') as file:\n",
    "        # prettify the soup object and convert it into a string\n",
    "        #file.write(str(soup.prettify()))\n",
    "        file.write(str(webpage))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "#print('Classes of each table:')\n",
    "#for table in soup.find_all('table'):\n",
    "#    print(table.get('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "html_tables = {}\n",
    "\n",
    "# For every table in the datasets folder...\n",
    "for table_name in os.listdir('data'):\n",
    "    #this is the path to the file\n",
    "    table_path = f'data/{table_name}'\n",
    "    # Open as a python file in read-only mode\n",
    "    table_file = open(table_path,\"r\")\n",
    "    # Read the contents of the file into 'html'\n",
    "    html_temp = table_file.read()\n",
    "    #html_temp2 = ''.join(format(ord(i), '08b') for i in html_temp)\n",
    "    html = BeautifulSoup(html_temp, \"html.parser\")\n",
    "    # Find 'news-table' in the Soup and load it into 'html_table'\n",
    "    html_table = html.find(\"table\",{\"id\":\"news-table\"})\n",
    "#    print(html_table)\n",
    "    # Add the table to our dictionary\n",
    "    html_tables[table_name] = html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Hold the parsed news into a list\n",
    "parsed_news = []\n",
    "# Iterate through the news\n",
    "for file_name, news_table in html_tables.items():\n",
    "    # Iterate through all tr tags in 'news_table'\n",
    "    for x in news_table.findAll('tr'):\n",
    "        # Read the text from the tr tag into text\n",
    "        try:\n",
    "            text = x.a.get_text()\n",
    "        except:\n",
    "            text = \"No Description\"\n",
    "        # Split the text in the td tag into a list\n",
    "        date_scrape = x.td.text.split()\n",
    "        #headline=x.td.text.split()\n",
    "        # If the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "        # If not, load 'date' as the 1st element and 'time' as the second\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "        # Extract the ticker from the file name, get the string up to the 1st '_'\n",
    "        #ticker =\n",
    "        ticker = file_name.split('.')[0]\n",
    "        # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "        parsed_news.append([ticker, date, time, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "# Set column names\n",
    "columns = ['ticker', 'date', 'time', 'headline']\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "# Iterate through the headlines and get the polarity scores using vader\n",
    "scores = parsed_and_scored_news['headline'].apply(vader.polarity_scores).tolist()\n",
    "# Convert the 'scores' list of dicts into a DataFrame\n",
    "scores_df = pd.DataFrame(scores)\n",
    "# Join the DataFrames of the news and the list of dicts\n",
    "parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')\n",
    "# Convert the date column from string to datetime\n",
    "parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "# Group by date and ticker columns from scored_news and calculate the mean\n",
    "mean_scores = parsed_and_scored_news.groupby(['ticker','date']).mean()\n",
    "# Unstack the column ticker\n",
    "mean_scores = mean_scores.unstack()\n",
    "# Get the cross-section of compound in the 'columns' axis\n",
    "mean_scores = mean_scores.xs('compound', axis=\"columns\").transpose()\n",
    "# Plot a bar chart\n",
    "#ax1 = plt.subplot2grid(shape=(2,6), loc=(0,0), colspan=2)\n",
    "#ax2 = plt.subplot2grid((2,6), (0,2), colspan=2)\n",
    "#ax3 = plt.subplot2grid((2,6), (0,4), colspan=2)\n",
    "#ax4 = plt.subplot2grid((2,6), (1,1), colspan=2)\n",
    "#ax5 = plt.subplot2grid((2,6), (1,3), colspan=2)\n",
    "color = ['r' if y<0 else 'g' for y in mean_scores['google']]\n",
    "#mean_scores.plot(y='google',kind = 'bar', color=color, legend= None)\n",
    "#plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f45e88d1bbb44a58e599194303aef5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Speed:', options=('Slow', 'Regular', 'Fast'), tooltips=('Description of slow', 'Des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#widgets.ToggleButtons(\n",
    "#    options=['Slow', 'Regular', 'Fast'],\n",
    "#    description='Speed:',\n",
    "#    disabled=False,\n",
    "#    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "#    tooltips=['Description of slow', 'Description of regular', 'Description of fast'],\n",
    "#    #     icons=['check'] * 3\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAANG Market Sentiment\n",
    "\n",
    "### This weekend project of mine scrapes financial websites for any news related to the FAANG companies, performs sentiment analysis on the news to provide a 'market mood' tracker for these stocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b77d1c7baa458db886e61992407f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(ToggleButtons(options=('Meta', 'Amazon', 'Apple', 'Netflix', 'Google'), value='Meta'), Output()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "\n",
    "\n",
    "buttons = widgets.ToggleButtons(\n",
    "    options=['Meta', 'Amazon', 'Apple', 'Netflix', 'Google'],\n",
    ")\n",
    "\n",
    "output=widgets.Output()\n",
    "def on_click(change):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        if (change['new']=='Meta'):           \n",
    "            mean_scores.plot(y='meta',kind = 'bar', color=color, legend= None)\n",
    "        elif (change['new']=='Amazon'):\n",
    "              mean_scores.plot(y='amazon',kind = 'bar', color=color, legend= None)\n",
    "        elif (change['new']=='Apple'):\n",
    "              mean_scores.plot(y='apple',kind = 'bar', color=color, legend= None)\n",
    "        elif (change['new']=='Netflix'):\n",
    "              mean_scores.plot(y='netflix',kind = 'bar', color=color, legend= None)\n",
    "        elif (change['new']=='Google'):\n",
    "              mean_scores.plot(y='google',kind = 'bar', color=color, legend= None)\n",
    "        show()\n",
    "\n",
    "buttons.observe(on_click, 'value')\n",
    "\n",
    "widgets.VBox([buttons, output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
